import gradio as gr
import torch
from transformers import pipeline
from huggingface_hub import list_models, ModelFilter
import logging

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class HuggingFaceInference:
    def __init__(self):
        self.current_pipeline = None
        self.current_task = None
        self.device = 0 if torch.cuda.is_available() else -1
        logger.info(f"ä½¿ç”¨è®¾å¤‡: {'GPU' if self.device == 0 else 'CPU'}")
    
    def get_available_tasks(self):
        """è·å–å¯ç”¨çš„ä»»åŠ¡ç±»å‹"""
        return [
            "æ–‡æœ¬åˆ†ç±»", 
            "æ–‡æœ¬ç”Ÿæˆ", 
            "é—®ç­”ç³»ç»Ÿ", 
            "æƒ…æ„Ÿåˆ†æ", 
            "å‘½åå®ä½“è¯†åˆ«",
            "ç¿»è¯‘"
        ]
    
    def get_models_by_task(self, task):
        """æ ¹æ®ä»»åŠ¡è·å–å¯ç”¨çš„æ¨¡å‹"""
        task_map = {
            "æ–‡æœ¬åˆ†ç±»": "text-classification",
            "æ–‡æœ¬ç”Ÿæˆ": "text-generation",
            "é—®ç­”ç³»ç»Ÿ": "question-answering",
            "æƒ…æ„Ÿåˆ†æ": "sentiment-analysis",
            "å‘½åå®ä½“è¯†åˆ«": "token-classification",
            "ç¿»è¯‘": "translation"
        }
        
        try:
            hf_task = task_map.get(task, "text-classification")
            models = list_models(filter=ModelFilter(task=hf_task), limit=10)
            return [model.modelId for model in models]
        except Exception as e:
            logger.error(f"è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: {e}")
            return ["distilbert-base-uncased-finetuned-sst-2-english"]
    
    def load_model(self, task, model_name):
        """åŠ è½½æŒ‡å®šçš„æ¨¡å‹"""
        task_map = {
            "æ–‡æœ¬åˆ†ç±»": "text-classification",
            "æ–‡æœ¬ç”Ÿæˆ": "text-generation", 
            "é—®ç­”ç³»ç»Ÿ": "question-answering",
            "æƒ…æ„Ÿåˆ†æ": "sentiment-analysis",
            "å‘½åå®ä½“è¯†åˆ«": "token-classification",
            "ç¿»è¯‘": "translation"
        }
        
        try:
            hf_task = task_map.get(task, "text-classification")
            logger.info(f"æ­£åœ¨åŠ è½½æ¨¡å‹: {model_name} ç”¨äºä»»åŠ¡: {hf_task}")
            
            self.current_pipeline = pipeline(
                hf_task,
                model=model_name,
                device=self.device
            )
            self.current_task = task
            logger.info(f"æ¨¡å‹åŠ è½½æˆåŠŸ: {model_name}")
            return f"æ¨¡å‹ {model_name} åŠ è½½æˆåŠŸï¼"
        except Exception as e:
            logger.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return f"æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}"
    
    def inference(self, text, context=None):
        """æ‰§è¡Œæ¨ç†"""
        if self.current_pipeline is None:
            return "è¯·å…ˆåŠ è½½æ¨¡å‹ï¼"
        
        try:
            if self.current_task == "é—®ç­”ç³»ç»Ÿ" and context:
                result = self.current_pipeline(question=text, context=context)
            elif self.current_task == "ç¿»è¯‘":
                result = self.current_pipeline(text)
            else:
                result = self.current_pipeline(text)
            
            logger.info(f"æ¨ç†å®Œæˆ: {result}")
            return self.format_result(result)
        except Exception as e:
            logger.error(f"æ¨ç†å¤±è´¥: {e}")
            return f"æ¨ç†å¤±è´¥: {str(e)}"
    
    def format_result(self, result):
        """æ ¼å¼åŒ–æ¨ç†ç»“æœ"""
        if isinstance(result, list):
            formatted = []
            for item in result:
                if isinstance(item, dict):
                    formatted.append("\n".join([f"{k}: {v}" for k, v in item.items()]))
                else:
                    formatted.append(str(item))
            return "\n\n".join(formatted)
        elif isinstance(result, dict):
            return "\n".join([f"{k}: {v}" for k, v in result.items()])
        else:
            return str(result)

# åˆ›å»ºæ¨ç†å®ä¾‹
inference_engine = HuggingFaceInference()

# åˆ›å»ºGradioç•Œé¢
def update_model_dropdown(task):
    """æ›´æ–°æ¨¡å‹ä¸‹æ‹‰æ¡†é€‰é¡¹"""
    models = inference_engine.get_models_by_task(task)
    return gr.Dropdown(choices=models, value=models[0] if models else "")

def load_model_and_update_status(task, model_name):
    """åŠ è½½æ¨¡å‹å¹¶æ›´æ–°çŠ¶æ€"""
    status = inference_engine.load_model(task, model_name)
    return status

def perform_inference(text, context):
    """æ‰§è¡Œæ¨ç†"""
    return inference_engine.inference(text, context)

# åˆ›å»ºç•Œé¢
with gr.Blocks(title="Hugging Face æ¨ç†æ¡†æ¶", theme=gr.themes.Soft()) as demo:
    gr.Markdown("# ğŸ¤— Hugging Face æœºå™¨å­¦ä¹ æ¨ç†æ¡†æ¶")
    gr.Markdown("é€‰æ‹©ä»»åŠ¡ç±»å‹å’Œæ¨¡å‹ï¼Œè¾“å…¥æ–‡æœ¬è¿›è¡Œæ¨ç†")
    
    with gr.Row():
        with gr.Column(scale=1):
            task_dropdown = gr.Dropdown(
                choices=inference_engine.get_available_tasks(),
                label="é€‰æ‹©ä»»åŠ¡ç±»å‹",
                value="æ–‡æœ¬åˆ†ç±»"
            )
            
            model_dropdown = gr.Dropdown(
                label="é€‰æ‹©æ¨¡å‹",
                value="Qwen/Qwen3-0.6B"
            )
            
            load_btn = gr.Button("åŠ è½½æ¨¡å‹", variant="primary")
            model_status = gr.Textbox(label="æ¨¡å‹çŠ¶æ€", interactive=False)
            
            context_input = gr.Textbox(
                label="ä¸Šä¸‹æ–‡ (ä»…é—®ç­”ç³»ç»Ÿéœ€è¦)",
                placeholder="å¯¹äºé—®ç­”ä»»åŠ¡ï¼Œè¯·åœ¨è¿™é‡Œè¾“å…¥ä¸Šä¸‹æ–‡...",
                lines=3
            )
        
        with gr.Column(scale=2):
            text_input = gr.Textbox(
                label="è¾“å…¥æ–‡æœ¬",
                placeholder="è¯·è¾“å…¥è¦æ¨ç†çš„æ–‡æœ¬...",
                lines=5
            )
            
            inference_btn = gr.Button("æ‰§è¡Œæ¨ç†", variant="primary")
            
            output = gr.Textbox(
                label="æ¨ç†ç»“æœ",
                lines=10,
                interactive=False
            )
    
    # äº‹ä»¶å¤„ç†
    task_dropdown.change(
        update_model_dropdown,
        inputs=task_dropdown,
        outputs=model_dropdown
    )
    
    load_btn.click(
        load_model_and_update_status,
        inputs=[task_dropdown, model_dropdown],
        outputs=model_status
    )
    
    inference_btn.click(
        perform_inference,
        inputs=[text_input, context_input],
        outputs=output
    )
    
    # åˆå§‹åŒ–æ¨¡å‹ä¸‹æ‹‰æ¡†
    demo.load(
        update_model_dropdown,
        inputs=task_dropdown,
        outputs=model_dropdown
    )

if __name__ == "__main__":
    # å¯åŠ¨æœåŠ¡
    demo.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False
    )